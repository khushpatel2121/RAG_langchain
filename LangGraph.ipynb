{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question(state):\n",
    "    llm =  ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-1.5-flash\",\n",
    "                    temperature=0,\n",
    "                    max_tokens=None,\n",
    "                    timeout=None,\n",
    "                    max_retries=2,\n",
    "                    api_key=os.getenv(\"GEMINI\")\n",
    "                )\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are an agent that needs to define if a question is a technical code one or a general one.\n",
    "\n",
    "    Question : {input}\n",
    "\n",
    "    Analyse the question. Only answer with \"code\" if the question is about technical development. If not just answer \"general\".\n",
    "\n",
    "    Your answer (code/general) :\n",
    "    \"\"\")\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"input\": state[\"input\"]})\n",
    "    decision = response.content.strip().lower()\n",
    "    return {\"decision\": decision, \"input\": state[\"input\"]}\n",
    "\n",
    "# Creating the generic agent\n",
    "def answer_generic_question(state):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-1.5-flash\",\n",
    "                    temperature=0,\n",
    "                    max_tokens=None,\n",
    "                    timeout=None,\n",
    "                    max_retries=2,\n",
    "                    api_key=os.getenv(\"GEMINI\")\n",
    "                )\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"Give a general and concise answer to the question: {input}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"input\": state[\"input\"]})\n",
    "    return {\"output\": response}\n",
    "\n",
    "def answer_code_question(state):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-1.5-flash\",\n",
    "                    temperature=0,\n",
    "                    max_tokens=None,\n",
    "                    timeout=None,\n",
    "                    max_retries=2,\n",
    "                    api_key=os.getenv(\"GEMINI\")\n",
    "                )\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"You are a software engineer. Answer this question with step by steps details : {input}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"input\": state[\"input\"]})\n",
    "    return {\"output\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "    decision: str\n",
    "\n",
    "#Here is a simple 3 steps graph that is going to be working in the bellow \"decision\" condition\n",
    "def create_graph():\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"analyze\", analyze_question)\n",
    "    workflow.add_node(\"code_agent\", answer_code_question)\n",
    "    workflow.add_node(\"generic_agent\", answer_generic_question)\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze\",\n",
    "        lambda x: x[\"decision\"],\n",
    "        {\n",
    "            \"code\": \"code_agent\",\n",
    "            \"general\": \"generic_agent\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.set_entry_point(\"analyze\")\n",
    "    workflow.add_edge(\"code_agent\", END)\n",
    "    workflow.add_edge(\"generic_agent\", END)\n",
    "    \n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722410491.136992 4704403 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1722410491.154872 4704403 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1722410500.866881 4704403 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final answer ---\n",
      "content='Canberra \\n' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-2dc2070f-3191-4c56-b4bd-8ff2d3bd7840-0' usage_metadata={'input_tokens': 20, 'output_tokens': 2, 'total_tokens': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1722410531.566826 4704876 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n",
      "I0000 00:00:1722410533.747453 4704876 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final answer ---\n",
      "content='Here\\'s how to create a simple \"Hello World\" webpage using HTML:\\n\\n**1. Create a New HTML File**\\n\\n* Open a text editor (like Notepad, Sublime Text, or VS Code).\\n* Create a new file and save it with a `.html` extension (e.g., `hello.html`).\\n\\n**2. Basic HTML Structure**\\n\\n* Add the following code to your `hello.html` file:\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n<head>\\n  <title>Hello World</title>\\n</head>\\n<body>\\n  <h1>Hello World!</h1>\\n</body>\\n</html>\\n```\\n\\n**Explanation:**\\n\\n* **`<!DOCTYPE html>`:** This line tells the browser that the document is an HTML5 document.\\n* **`<html>`:** The root element of the HTML document.\\n* **`<head>`:** Contains metadata about the page (like the title).\\n* **`<title>`:** Sets the title that appears in the browser tab.\\n* **`<body>`:** Contains the visible content of the webpage.\\n* **`<h1>`:** An HTML heading tag (the largest heading).\\n\\n**3. Save and Open the File**\\n\\n* Save your `hello.html` file.\\n* Open the file in your web browser (by double-clicking it or dragging it into the browser window).\\n\\n**You should now see a webpage with the words \"Hello World!\" displayed as a heading.**\\n\\n**Additional Tips:**\\n\\n* **Styling:** You can use CSS to style the appearance of your \"Hello World\" message. Add a `<style>` tag within the `<head>` section to include CSS rules.\\n* **More Content:** Add more HTML elements to your page, such as paragraphs (`<p>`), images (`<img>`), or links (`<a>`).\\n* **Web Server:** To make your webpage accessible on the internet, you\\'ll need to host it on a web server. There are free hosting options available online.\\n\\nLet me know if you\\'d like to explore any of these additional features! \\n' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-91f61098-1a69-4670-9f9e-a1583109a456-0' usage_metadata={'input_tokens': 31, 'output_tokens': 443, 'total_tokens': 474}\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class UserInput(TypedDict):\n",
    "    input: str\n",
    "    continue_conversation: bool\n",
    "\n",
    "def get_user_input(state: UserInput) -> UserInput:\n",
    "    user_input = input(\"\\nEnter your question (ou 'q' to quit) : \")\n",
    "    state[\"input\"] = user_input\n",
    "    state[\"continue_conversation\"] = user_input.lower() != 'q'\n",
    "    return state\n",
    "\n",
    "def process_question(state: UserInput):\n",
    "    graph = create_graph()\n",
    "    result = graph.invoke({\"input\": state[\"input\"]})\n",
    "    print(\"\\n--- Final answer ---\")\n",
    "    print(result[\"output\"])\n",
    "    return state\n",
    "\n",
    "def create_conversation_graph():\n",
    "    workflow = StateGraph(UserInput)\n",
    "\n",
    "    workflow.add_node(\"get_input\", get_user_input)\n",
    "    workflow.add_node(\"process_question\", process_question)\n",
    "\n",
    "    workflow.set_entry_point(\"get_input\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"get_input\",\n",
    "        lambda x: \"continue\" if x[\"continue_conversation\"] else \"end\",\n",
    "        {\n",
    "            \"continue\": \"process_question\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"process_question\", \"get_input\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "def main():\n",
    "    conversation_graph = create_conversation_graph()\n",
    "    conversation_graph.invoke({\"input\": \"\", \"continue_conversation\": True})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
